{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgist/miniconda3/envs/HSE/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from regressCNN import RegressionPCA\n",
    "\n",
    "from data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print('cuda available:', cuda_available)\n",
    "\n",
    "device = torch.device('cuda:0' if cuda_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_data(x):\n",
    "    row, col = x.shape\n",
    "    data = np.zeros([row, col+2])\n",
    "    data[:, 0] = x[:, -1]\n",
    "    data[:, 1:-1] = x\n",
    "    data[:, -1] = x[:, 0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSEDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.frontal = []\n",
    "        self.lateral = []\n",
    "        self.shape = []\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        for dir in tqdm(os.listdir(data_path), desc='loading data'):\n",
    "            path = os.path.join(data_path, dir)\n",
    "            f = np.load(os.path.join(path, 'frontal.npy'))\n",
    "            l = np.load(os.path.join(path, 'lateral.npy'))\n",
    "            s = np.load(os.path.join(path, 'shape.npy'))\n",
    "\n",
    "            self.frontal.append(f)\n",
    "            self.lateral.append(l)\n",
    "            self.shape.append(s)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        f = self.frontal[i]\n",
    "        l = self.lateral[i]\n",
    "        s = self.shape[i]\n",
    "\n",
    "        f = repeat_data(f.transpose())\n",
    "        l = repeat_data(l.transpose())\n",
    "        return f, l, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "training_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data: 100%|██████████| 7970/7970 [00:09<00:00, 866.35it/s]\n",
      "loading data: 100%|██████████| 886/886 [00:01<00:00, 866.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader len: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HSEDataset('../../dataset-generation/datasets/HSE/sample_648/train/')\n",
    "test_dataset = HSEDataset('../../dataset-generation/datasets/HSE/sample_648/test/')\n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('train dataloader len:', len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionPCA(22).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1  TrainLoss:93.343548\n",
      "epoch:2  TrainLoss:85.344968\n",
      "epoch:3  TrainLoss:79.795928\n",
      "epoch:4  TrainLoss:75.680061\n",
      "epoch:5  TrainLoss:71.171241\n",
      "epoch:6  TrainLoss:66.852079\n",
      "epoch:7  TrainLoss:66.821056\n",
      "epoch:8  TrainLoss:65.454727\n",
      "epoch:9  TrainLoss:62.072293\n",
      "epoch:10  TrainLoss:60.207829\n",
      "epoch:11  TrainLoss:59.128567\n",
      "epoch:12  TrainLoss:58.119415\n",
      "epoch:13  TrainLoss:57.383751\n",
      "epoch:14  TrainLoss:56.436361\n",
      "epoch:15  TrainLoss:54.947428\n",
      "epoch:16  TrainLoss:53.587152\n",
      "epoch:17  TrainLoss:51.587487\n",
      "epoch:18  TrainLoss:51.800449\n",
      "epoch:19  TrainLoss:52.042034\n",
      "epoch:20  TrainLoss:53.202320\n",
      "epoch:21  TrainLoss:50.290330\n",
      "epoch:22  TrainLoss:50.315505\n",
      "epoch:23  TrainLoss:50.927830\n",
      "epoch:24  TrainLoss:49.509807\n",
      "epoch:25  TrainLoss:47.537525\n",
      "epoch:26  TrainLoss:47.127636\n",
      "epoch:27  TrainLoss:46.550761\n",
      "epoch:28  TrainLoss:47.333080\n",
      "epoch:29  TrainLoss:47.336948\n",
      "epoch:30  TrainLoss:45.100973\n",
      "epoch:31  TrainLoss:44.976434\n",
      "epoch:32  TrainLoss:44.132384\n",
      "epoch:33  TrainLoss:43.096367\n",
      "epoch:34  TrainLoss:43.183855\n",
      "epoch:35  TrainLoss:42.689362\n",
      "epoch:36  TrainLoss:43.289169\n",
      "epoch:37  TrainLoss:43.018103\n",
      "epoch:38  TrainLoss:43.642538\n",
      "epoch:39  TrainLoss:41.784117\n",
      "epoch:40  TrainLoss:41.361854\n",
      "epoch:41  TrainLoss:40.348085\n",
      "epoch:42  TrainLoss:40.127443\n",
      "epoch:43  TrainLoss:41.046193\n",
      "epoch:44  TrainLoss:41.674868\n",
      "epoch:45  TrainLoss:39.854457\n",
      "epoch:46  TrainLoss:38.854699\n",
      "epoch:47  TrainLoss:37.918013\n",
      "epoch:48  TrainLoss:38.858236\n",
      "epoch:49  TrainLoss:37.305788\n",
      "epoch:50  TrainLoss:37.210959\n",
      "epoch:51  TrainLoss:36.383018\n",
      "epoch:52  TrainLoss:36.195419\n",
      "epoch:53  TrainLoss:35.724629\n",
      "epoch:54  TrainLoss:37.133409\n",
      "epoch:55  TrainLoss:35.794234\n",
      "epoch:56  TrainLoss:36.022293\n",
      "epoch:57  TrainLoss:34.586650\n",
      "epoch:58  TrainLoss:35.124693\n",
      "epoch:59  TrainLoss:33.863842\n",
      "epoch:60  TrainLoss:35.851030\n",
      "epoch:61  TrainLoss:33.916093\n",
      "epoch:62  TrainLoss:34.442442\n",
      "epoch:63  TrainLoss:34.241926\n",
      "epoch:64  TrainLoss:33.800165\n",
      "epoch:65  TrainLoss:33.662753\n",
      "epoch:66  TrainLoss:31.698453\n",
      "epoch:67  TrainLoss:32.779741\n",
      "epoch:68  TrainLoss:33.014803\n",
      "epoch:69  TrainLoss:31.539375\n",
      "epoch:70  TrainLoss:31.345514\n",
      "epoch:71  TrainLoss:31.618351\n",
      "epoch:72  TrainLoss:31.334176\n",
      "epoch:73  TrainLoss:33.088419\n",
      "epoch:74  TrainLoss:32.220748\n",
      "epoch:75  TrainLoss:30.560237\n",
      "epoch:76  TrainLoss:29.849764\n",
      "epoch:77  TrainLoss:29.718740\n",
      "epoch:78  TrainLoss:31.315042\n",
      "epoch:79  TrainLoss:29.491261\n",
      "epoch:80  TrainLoss:29.912880\n",
      "epoch:81  TrainLoss:30.783686\n",
      "epoch:82  TrainLoss:30.121213\n",
      "epoch:83  TrainLoss:31.409964\n",
      "epoch:84  TrainLoss:27.691368\n",
      "epoch:85  TrainLoss:27.903700\n",
      "epoch:86  TrainLoss:28.414934\n",
      "epoch:87  TrainLoss:27.345955\n",
      "epoch:88  TrainLoss:26.884561\n",
      "epoch:89  TrainLoss:27.091302\n",
      "epoch:90  TrainLoss:29.102498\n",
      "epoch:91  TrainLoss:27.332568\n",
      "epoch:92  TrainLoss:27.976299\n",
      "epoch:93  TrainLoss:28.460806\n",
      "epoch:94  TrainLoss:26.620465\n",
      "epoch:95  TrainLoss:26.346195\n",
      "epoch:96  TrainLoss:26.849483\n",
      "epoch:97  TrainLoss:25.654060\n",
      "epoch:98  TrainLoss:25.568105\n",
      "epoch:99  TrainLoss:29.160216\n",
      "epoch:100  TrainLoss:25.454945\n",
      "epoch:101  TrainLoss:27.387124\n",
      "epoch:102  TrainLoss:26.553570\n",
      "epoch:103  TrainLoss:26.320904\n",
      "epoch:104  TrainLoss:25.100825\n",
      "epoch:105  TrainLoss:23.633075\n",
      "epoch:106  TrainLoss:24.973297\n",
      "epoch:107  TrainLoss:24.921445\n",
      "epoch:108  TrainLoss:24.616112\n",
      "epoch:109  TrainLoss:24.632040\n",
      "epoch:110  TrainLoss:25.590444\n",
      "epoch:111  TrainLoss:26.766361\n",
      "epoch:112  TrainLoss:23.646311\n"
     ]
    }
   ],
   "source": [
    "gender='male'\n",
    "\n",
    "fig_loss_train = []\n",
    "fig_epoch = []\n",
    "for epoch in range(1, training_epoch+1):\n",
    "    loss_n = 0\n",
    "    for i, data_temp in enumerate(train_data_loader):\n",
    "        # read data from data_loader, get 32 data each time\n",
    "        front_e_ith, side_e_ith, shape_ith = data_temp\n",
    "        front_e_ith, side_e_ith, shape_ith = \\\n",
    "            front_e_ith.to(device, dtype = torch.float), \\\n",
    "                side_e_ith.to(device, dtype = torch.float), \\\n",
    "                    shape_ith.to(device, dtype = torch.float)\n",
    "        # feed data and forward pass\n",
    "        outputs = model(front_e_ith, side_e_ith)\n",
    "        #********************************************************************************************\n",
    "        loss = criterion(outputs, shape_ith.float())\n",
    "        #print(loss.item())\n",
    "        #temp_e = shape_error(outputs, shape_ith.float())\n",
    "        #print(temp_e)\n",
    "        #loss_n += temp_e\n",
    "        loss_n += loss.item()\n",
    "        #********************************************************************************************\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # if epoch%5 == 0:\n",
    "    #     print('epoch:{}  TrainLoss:{:.6f}'.format(epoch, loss_n))\n",
    "    #     fig_loss_train = fig_loss_train + [loss_n]\n",
    "    #     fig_epoch = fig_epoch + [epoch]\n",
    "    #     torch.save(model.state_dict(), './model/model_{}_{}.ckpt'.format(gender, epoch))\n",
    "    print('epoch:{}  TrainLoss:{:.6f}'.format(epoch, loss_n))\n",
    "    fig_loss_train = fig_loss_train + [loss_n]\n",
    "    fig_epoch = fig_epoch + [epoch]\n",
    "    if epoch%10 == 0:\n",
    "        torch.save(model.state_dict(), './model/model_{}_{}.ckpt'.format(gender, epoch))\n",
    "\n",
    "# np.save('./data/output/{}_fig_loss_train.npy'.format(gender), np.array(fig_loss_train))\n",
    "\n",
    "plt.plot(fig_epoch, fig_loss_train)\n",
    "plt.title('training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7034974618415747\n"
     ]
    }
   ],
   "source": [
    "model_name = './model/model_male_100.ckpt'\n",
    "infer_model = RegressionPCA(22).to(device)\n",
    "infer_model.load_state_dict(torch.load(model_name))\n",
    "infer_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data_temp in enumerate(test_data_loader):\n",
    "        # read data from data_loader, get 32 data each time\n",
    "        front_e_ith, side_e_ith, shape_ith = data_temp\n",
    "        front_e_ith, side_e_ith, shape_ith = \\\n",
    "            front_e_ith.to(device, dtype = torch.float), \\\n",
    "                side_e_ith.to(device, dtype = torch.float), \\\n",
    "                    shape_ith.to(device, dtype = torch.float)\n",
    "        # feed data and forward pass\n",
    "        outputs = infer_model(front_e_ith, side_e_ith)\n",
    "        #********************************************************************************************\n",
    "        loss = criterion(outputs, shape_ith.float())\n",
    "        #print(loss.item())\n",
    "        #temp_e = shape_error(outputs, shape_ith.float())\n",
    "        #print(temp_e)\n",
    "        #loss_n += temp_e\n",
    "        loss_n += loss.item()\n",
    "        #********************************************************************************************\n",
    "        # backward and optimize\n",
    "\n",
    "print(loss_n / len(test_data_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
